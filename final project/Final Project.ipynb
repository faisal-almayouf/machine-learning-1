{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1:  Summarize for us the goal of this project and how machine learning is useful in trying to accomplish it. As part of your answer, give some background on the dataset and how it can be used to answer the project question. Were there any outliers in the data when you got it, and how did you handle those?\n",
    "\n",
    "the company 'Enron' back in 2000 was known as one of the largest energy companies in the United States, with around $111 billion in revenues. At the end of 2001, it was revealed that there was widespread corporate fraud. Essentially all of Enron's nonexistent profits were created through a method called \"mark-to-market\" accounting, in which they would report profits even though they didn't earn a single dime. Even worse, they were solely responsible for the California electricity crisis, by which they performed large-scale blackouts to seize arbitrage opportunities.\n",
    "###### The goal for this project is to analyze the characteristics of employees who worked at Enron during the crisis to  predict if they are a person of interest (\"POI\")\n",
    "###### * this dataset contains 146 entries.\n",
    "###### * there are some outliers in the data, i plotted the salaries and bonus data to have an illustration about the distribution of the data, then i calculated a vlaue (97.5 percentile) to compare its value with the field value and return if it has higher score, some outliers represent an executive level so i decided to keep them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 146 entries, ALLEN PHILLIP K to YEAP SOON\n",
      "Data columns (total 18 columns):\n",
      "poi                          146 non-null bool\n",
      "salary                       95 non-null float64\n",
      "bonus                        82 non-null float64\n",
      "deferred_income              49 non-null float64\n",
      "director_fees                17 non-null float64\n",
      "exercised_stock_options      102 non-null float64\n",
      "expenses                     95 non-null float64\n",
      "loan_advances                4 non-null float64\n",
      "long_term_incentive          66 non-null float64\n",
      "restricted_stock             110 non-null float64\n",
      "restricted_stock_deferred    18 non-null float64\n",
      "shared_receipt_with_poi      86 non-null float64\n",
      "total_payments               125 non-null float64\n",
      "total_stock_value            126 non-null float64\n",
      "from_messages                86 non-null float64\n",
      "from_poi_to_this_person      86 non-null float64\n",
      "from_this_person_to_poi      86 non-null float64\n",
      "to_messages                  86 non-null float64\n",
      "dtypes: bool(1), float64(17)\n",
      "memory usage: 20.7+ KB\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python\n",
    "\n",
    "import sys\n",
    "import pickle\n",
    "sys.path.append(\"../tools/\")\n",
    "import scipy\n",
    "import pandas\n",
    "import numpy\n",
    "import matplotlib\n",
    "import tester\n",
    "from matplotlib import pyplot\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "from tester import dump_classifier_and_data\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn import tree\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from tester import test_classifier, dump_classifier_and_data\n",
    "\n",
    "### Task 1: Select what features you'll use.\n",
    "### features_list is a list of strings, each of which is a feature name.\n",
    "### The first feature must be \"poi\".\n",
    "features_list = ['poi', 'salary',\n",
    "                 'bonus',\n",
    "  'deferred_income',\n",
    "  'director_fees',\n",
    "  'exercised_stock_options',\n",
    "  'expenses',\n",
    "  'loan_advances',\n",
    "  'long_term_incentive',\n",
    "  'restricted_stock',\n",
    "  'restricted_stock_deferred',\n",
    "  'shared_receipt_with_poi',\n",
    "  'total_payments',\n",
    "  'total_stock_value',\n",
    "'from_messages',\n",
    "'from_poi_to_this_person',\n",
    "  'from_this_person_to_poi',\n",
    "    'to_messages'] # You will need to use more features\n",
    "\n",
    "### Load the dictionary containing the dataset\n",
    "with open(\"final_project_dataset.pkl\", \"r\") as data_file:\n",
    "    data_dict = pickle.load(data_file)\n",
    "my_dataset = data_dict\n",
    "\n",
    "\n",
    "\n",
    "##store the data in a data frame\n",
    "df=pandas.DataFrame.from_dict(my_dataset, orient = 'index')\n",
    "\n",
    "df= df[features_list]\n",
    "df = df.replace('NaN', numpy.nan)\n",
    "df.info()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of Null Values per feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "poi                            0\n",
       "salary                        51\n",
       "bonus                         64\n",
       "deferred_income               97\n",
       "director_fees                129\n",
       "exercised_stock_options       44\n",
       "expenses                      51\n",
       "loan_advances                142\n",
       "long_term_incentive           80\n",
       "restricted_stock              36\n",
       "restricted_stock_deferred    128\n",
       "shared_receipt_with_poi       60\n",
       "total_payments                21\n",
       "total_stock_value             20\n",
       "from_messages                 60\n",
       "from_poi_to_this_person       60\n",
       "from_this_person_to_poi       60\n",
       "to_messages                   60\n",
       "dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum(axis=0) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of POI's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of POI's: 18\n"
     ]
    }
   ],
   "source": [
    "pois = [x for x, y in my_dataset.items() if y['poi']]\n",
    "print 'Number of POI\\'s: {0}'.format(len(pois))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### total NaN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total NaN values:  1163\n"
     ]
    }
   ],
   "source": [
    "print \"total NaN values: \", df.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### we will fill the null numeric values with 0 for better analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.ix[:,:14] = df.ix[:,:14].fillna(0)\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### for mail features, i wil split them in two calsses, then the missing values will be filled with the meadian of each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "email_features = [ 'from_messages', 'from_this_person_to_poi', 'from_poi_to_this_person','to_messages']\n",
    "\n",
    "imp = Imputer(missing_values='NaN', strategy='median', axis=0)\n",
    "\n",
    "#impute missing values of email features \n",
    "df.loc[df[df.poi == 1].index,email_features] = imp.fit_transform(df[email_features][df.poi == 1])\n",
    "df.loc[df[df.poi == 0].index,email_features] = imp.fit_transform(df[email_features][df.poi == 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlier invesegation :\n",
    "\n",
    "#### For this part, i will plot the salary vs bonus data to show the distribution of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAERCAYAAACU1LsdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xc5f45c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "    \n",
    "#   \n",
    "# as an example, i plotted the salary vs bonus \n",
    "#\n",
    "\n",
    "\n",
    "for dic in data_dict.values():\n",
    "    \n",
    "   matplotlib.pyplot.scatter( dic['salary'] , dic['bonus']  )\n",
    "\n",
    "matplotlib.pyplot.xlabel(\"salary\")\n",
    "matplotlib.pyplot.ylabel(\"bonus\")\n",
    "matplotlib.pyplot.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### next i will calculate the salaries outlier, i will assume that the highest 2.5% percent are the outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the salaries outlier is :807247.625\n",
      "\n",
      "\n",
      "name : LAY KENNETH L :: salary 1072321\n",
      "name : SKILLING JEFFREY K :: salary 1111258\n",
      "name : TOTAL :: salary 26704229\n",
      "name : FREVERT MARK A :: salary 1060932\n",
      "\n",
      "\n",
      "the bonus outlier is :5381249.375\n",
      "\n",
      "\n",
      "name : LAVORATO JOHN J :: bonus 8000000\n",
      "name : LAY KENNETH L :: bonus 7000000\n",
      "name : SKILLING JEFFREY K :: bonus 5600000\n",
      "name : TOTAL :: bonus 97343619\n"
     ]
    }
   ],
   "source": [
    "# the plot shows a huge outlier, but lets check out the outler for the salary field\n",
    "\n",
    "outliers = df.quantile(.975) \n",
    "print 'the salaries outlier is :{}'.format(outliers[1])\n",
    "        \n",
    "print('\\n') \n",
    "\n",
    "for k, v in data_dict.items():\n",
    "    if v['salary'] != 'NaN' and v['salary'] > outliers[1]: print 'name : {}'.format(k),':: salary {}'.format(v['salary'])\n",
    "print('\\n')\n",
    "\n",
    "outliers = df.quantile(.975) \n",
    "print 'the bonus outlier is :{}'.format(outliers[2])\n",
    "print('\\n') \n",
    "for k, v in data_dict.items():\n",
    "    if v['bonus'] != 'NaN' and v['bonus'] > outliers[2]: print 'name : {}'.format(k),':: bonus {}'.format(v['bonus'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 'Kenneth Lee' , 'Jeffrey Skilling' and 'FREVERT MARK A'  are very well known persons from ENRON, so i shouldn't exclude them.\n",
    "##### 'LAVORATO JOHN J' on the other hand isn't as famous as the others and as i read his bonus was more of a ''company way to retain its talents\" .\n",
    "##### the value 'TOTAL'  looks to be the outlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# remove the value total  \n",
    "df = df.drop(['TOTAL','LAVORATO JOHN J'],0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create new feature(s)\n",
    "#### in this part i will create two featuers, both shows the recentage of the emails between this person and a POI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "df_OLD = df[features_list] ## Keep the original data frame with the old features\n",
    "\n",
    "## new feature that shows the amount of fraction of mails sent to a poi\n",
    "df['fraction_to_poi'] = df['from_this_person_to_poi']/df['from_messages']\n",
    "\n",
    "## new feature that shows the amount of fraction of mails sent from a poi\n",
    "df['fraction_from_poi']=df['from_poi_to_this_person'] / df['to_messages']\n",
    "\n",
    "#clean all 'infinite' values which ,\"division by zero\"\n",
    "df = df.replace('inf', 0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2: What features did you end up using in your POI identifier, and what selection process did you use to pick them? Did you have to do any scaling? Why or why not? As part of the assignment, you should attempt to engineer your own feature that does not come ready-made in the dataset -- explain what feature you tried to make, and the rationale behind it.\n",
    "\n",
    "###### * in the project i used Feature Importance in Decision Tree algorithm to get the best featuers,GridSearchCV is used to achieve the best estimate of importances of each feature.\n",
    "###### * i have created two new features ('fraction_to_poi','fraction_from_poi') to measure the communication between a person and a POI\n",
    "### Feature Scaling\n",
    "###### * for the feature selection desicion Tree algoritm i haven't done any scaling because Decision tree doesn't require me to do any feature scaling .\n",
    "######  *since i'm going to try three classifiers, for  $Decision Tree$ and $Random Forest$ i havent done any scaling and used the features optained from Feature importnace in Decision Tree, for  $GaussianNB$ i have done standardized features scaling to have a normal distributed features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seletcting  the features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First , i will check the old features importance ( without the new features) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['from_poi_to_this_person', 0.23228479361669468]\n",
      "['from_messages', 0.17138045074553013]\n",
      "['from_this_person_to_poi', 0.1467315876672603]\n",
      "['expenses', 0.12145089696110102]\n",
      "['bonus', 0.095238095238095233]\n",
      "['to_messages', 0.073881673881673909]\n",
      "['long_term_incentive', 0.057142857142857162]\n",
      "['shared_receipt_with_poi', 0.052910052910052914]\n",
      "['total_payments', 0.048979591836734629]\n"
     ]
    }
   ],
   "source": [
    "Tclf = tree.DecisionTreeClassifier(random_state = 42)\n",
    "Tclf.fit(df_OLD.ix[:,1:], df_OLD.ix[:,:1])\n",
    "\n",
    "# show the features with non null importance, sorted and create features_list of features for the model\n",
    "features_importance = []\n",
    "for i in range(len(Tclf.feature_importances_)):\n",
    "    if Tclf.feature_importances_[i] > 0:\n",
    "        features_importance.append([df_OLD.columns[i+1], Tclf.feature_importances_[i]])\n",
    "features_importance.sort(key=lambda x: x[1], reverse = True)\n",
    "for f_i in features_importance:\n",
    "    print f_i\n",
    "#features_list = [x[0] for x in features_importance]\n",
    "#features_list.insert(0, 'poi')\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next i the feature importance for all the features ( including the new ones )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fraction_to_poi', 0.34693877551020413]\n",
      "['expenses', 0.31131267321743517]\n",
      "['to_messages', 0.22010582010582005]\n",
      "['total_payments', 0.070546737213403862]\n",
      "['from_messages', 0.051095993953136812]\n"
     ]
    }
   ],
   "source": [
    "clf = tree.DecisionTreeClassifier(random_state = 42)\n",
    "clf.fit(df.ix[:,1:], df.ix[:,:1])\n",
    "\n",
    "# show the features with non null importance, sorted and create features_list of features for the model\n",
    "features_importance = []\n",
    "for i in range(len(clf.feature_importances_)):\n",
    "    if clf.feature_importances_[i] > 0:\n",
    "        features_importance.append([df.columns[i+1], clf.feature_importances_[i]])\n",
    "features_importance.sort(key=lambda x: x[1], reverse = True)\n",
    "for f_i in features_importance:\n",
    "    print f_i\n",
    "features_list = [x[0] for x in features_importance]\n",
    "features_list.insert(0, 'poi')\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### the result contains features with non-null feature importance, sorted by importance.\n",
    "#### the importance of the features changed after adding the new features, the new feature ''fraction_to_poi' got the highest score.\n",
    "#### the more features lead to overfitting the model, and less features leads to a bias model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_features_list=['poi','fraction_to_poi', \n",
    "'expenses', \n",
    "'to_messages',\n",
    "'total_payments',\n",
    "'from_messages' ]\n",
    "\n",
    "new_dataset=df.to_dict(orient='index')\n",
    "data = featureFormat(new_dataset, new_features_list)\n",
    "labels, features = targetFeatureSplit(data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3: What algorithm did you end up using? What other one(s) did you try? How did model performance differ between algorithms?\n",
    "\n",
    "###### i tried three classifires:\n",
    "###### 1-  GaussianNB                        (Precision: 0.24144\tRecall: 0.07050\t)\n",
    "###### 2-  RandomForestClassifier    (Precision: 0.61296\tRecall: 0.37850 )\n",
    "###### 3-  DecisionTreeClassifier       ( Precision: 0.61208\tRecall: 0.63350 )\n",
    "###  DecisionTreeClassifier has the best recall , and in our case the recall is more important since we want the algorithm to return all POI indivisual."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Classifier Selection\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tester import test_classifier, dump_classifier_and_data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "### Task 4: Try a varity of classifiers\n",
    "### Please name your classifier clf for easy export below.\n",
    "### Note that if you want to do PCA or other multi-stage operations,\n",
    "### you'll need to use Pipelines. For more info:\n",
    "### http://scikit-learn.org/stable/modules/pipeline.html\n",
    "\n",
    "\n",
    "\n",
    "# Provided to give you a starting point. Try a variety of classifiers.\n",
    "\n",
    "   \n",
    "#from sklearn.metrics import confusion_matrix, precision_score, recall_score, classification_report\n",
    "\n",
    "#############################################################################\n",
    "###  GaussianNB()                                                           #####\n",
    "###### Accuracy: 0.84653\tPrecision: 0.24144\tRecall: 0.07050\tF1: 0.10913 ######\n",
    " #############################################################################\n",
    "#data = featureFormat(new_dataset, features_list)\n",
    "#labels, features = targetFeatureSplit(data)\n",
    "#from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "#from sklearn.cross_validation import train_test_split\n",
    "#features_train, features_test, labels_train, labels_test = \\\n",
    "#   train_test_split(features, labels, test_size=0.3, random_state=42)\n",
    "\n",
    "\n",
    "#scaler = StandardScaler()\n",
    "#features = scaler.fit_transform(features)\n",
    "\n",
    "#clf = GaussianNB()\n",
    "#clf = clf.fit(features_train,labels_train)\n",
    "\n",
    "#test_classifier(clf, new_dataset, features_list)\n",
    "\n",
    "\n",
    "\n",
    " #############################################################################\n",
    "###  RandomForestClassifier                                                 #####\n",
    "###### Accuracy: 0.88527\tPrecision: 0.61296\tRecall: 0.37850\tF1: 0.46801 #####\n",
    " #############################################################################\n",
    "#clf=RandomForestClassifier()\n",
    "#clf.fit(features_train, labels_train)\n",
    "#pred=clf.predict(features_test)\n",
    "\n",
    " #############################################################################\n",
    "###  DecisionTreeClassifier                                                 #####\n",
    "###### Accuracy: 0.89760\tPrecision: 0.61208\tRecall: 0.63350\tF1: 0.62260 #####\n",
    " #############################################################################\n",
    "#clf=tree.DecisionTreeClassifier()\n",
    "#clf.fit(features_train, labels_train)\n",
    "#pred=clf.predict(features_test)\n",
    "\n",
    "#test_classifier(clf, new_dataset, features_list)\n",
    "#tester.dump_classifier_and_data(clf, new_dataset, features_list)\n",
    "#tester.main() \n",
    "\n",
    "### Task 5: Tune your classifier to achieve better than .3 precision and recall \n",
    "### using our testing script. Check the tester.py script in the final project\n",
    "### folder for details on the evaluation method, especially the test_classifier\n",
    "### function. Because of the small size of the dataset, the script uses\n",
    "### stratified shuffle split cross validation. For more info: \n",
    "### http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.StratifiedShuffleSplit.html\n",
    "\n",
    "# Example starting point. Try investigating other evaluation techniques!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4: What does it mean to tune the parameters of an algorithm, and what can happen if you don’t do this well? How did you tune the parameters of your particular algorithm?\n",
    "\n",
    "#### Parameters tuning is an important step, the default behavior of the fanction might not be always efficient, adjustment of the algorithm when training improves the fit on the test set.\n",
    "#### Parameter can influence the outcome of the learning process, the more tuned the parameters, the more biased the algorithm will be to the training data.\n",
    "##### in DecisionTreeClassifier i used the following parameters : (max_features=5,min_samples_split=2,max_depth = 10,criterion='gini',random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm tuning\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "clf = tree.DecisionTreeClassifier(max_features=5,min_samples_split=2,\n",
    "                                  max_depth = 10,criterion='gini',\n",
    "                                  random_state=42)\n",
    "                                  \n",
    "           \n",
    "\n",
    "#tester.dump_classifier_and_data(clf, new_dataset, features_list)\n",
    "#tester.main() \n",
    "\n",
    "### Task 5: Tune your classifier to achieve better than .3 precision and recall \n",
    "### using our testing script. Check the tester.py script in the final project\n",
    "### folder for details on the evaluation method, especially the test_classifier\n",
    "### function. Because of the small size of the dataset, the script uses\n",
    "### stratified shuffle split cross validation. For more info: \n",
    "### http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.StratifiedShuffleSplit.html\n",
    "\n",
    "# Example starting point. Try investigating other evaluation techniques!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<pre>\n",
    "```\n",
    "\n",
    "| Parameters                             | score              |\n",
    "|----------------------------------------|--------------------|\n",
    "| max_features=2,min_samples_split=5,    | Precision: 0.65414 |\n",
    "|  max_depth = 40,criterion='entropy',   |                    |\n",
    "|    random_state=75                     | Recall:    0.60050 | \n",
    "|-------------------------------------------------------------|\n",
    "| max_features=4,min_samples_split=2,    | Precision: 0.61693 |\n",
    "|  ,max_depth = 10,criterion='gini',     |                    |\n",
    "|    random_state=42                     | Recall:    0.67400 | \n",
    "|-------------------------------------------------------------|\n",
    "| max_features=3,min_samples_split=10,   | Precision: 0.69540 |\n",
    "|  ,max_depth = 50,criterion='entropy',  |                    |\n",
    "|    random_state=72                     | Recall:    0.59700 | \n",
    "|-------------------------------------------------------------|\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "</pre>\n",
    "\n",
    "### Next i will run the tuned algoritm with  train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 = 0.363636363636\n",
      "f2 = 0.285714285714\n",
      "Accuracy = 0.840909090909 Percision = 0.666666666667 Recall = 0.25\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "\n",
    "features_train, features_test, labels_train, labels_test = \\\n",
    "   train_test_split(features, labels, test_size=0.3, random_state=42)\n",
    "\n",
    "data = featureFormat(new_dataset, new_features_list)\n",
    "labels, features = targetFeatureSplit(data)\n",
    "\n",
    "clf = tree.DecisionTreeClassifier(max_features=2,min_samples_split=5,max_depth = 40,criterion='entropy',\n",
    "                                 random_state=75)\n",
    "\n",
    "true_negatives = 0\n",
    "false_negatives = 0\n",
    "true_positives = 0\n",
    "false_positives = 0\n",
    "\n",
    "        ### fit the classifier using training set, and test on test set\n",
    "clf.fit(features_train, labels_train)\n",
    "predictions = clf.predict(features_test)\n",
    "for prediction, truth in zip(predictions, labels_test):\n",
    "    if prediction == 0 and truth == 0:\n",
    "        true_negatives += 1\n",
    "    elif prediction == 0 and truth == 1:\n",
    "        false_negatives += 1\n",
    "    elif prediction == 1 and truth == 0:\n",
    "        false_positives += 1\n",
    "    elif prediction == 1 and truth == 1:\n",
    "        true_positives += 1\n",
    "    else:\n",
    "        print \"Warning: Found a predicted label not == 0 or 1.\"\n",
    "        print \"All predictions should take value 0 or 1.\"\n",
    "        print \"Evaluating performance for processed predictions:\"\n",
    "        break;\n",
    "total_predictions = true_negatives + false_negatives + false_positives + true_positives\n",
    "accuracy = 1.0*(true_positives + true_negatives)/total_predictions\n",
    "precision = 1.0*true_positives/(true_positives+false_positives)\n",
    "recall = 1.0*true_positives/(true_positives+false_negatives)\n",
    "print 'f1 =', 2.0 * true_positives/(2*true_positives + false_positives+false_negatives)\n",
    "print 'f2 =', (1+2.0*2.0) * precision*recall/(4*precision + recall)\n",
    "#print clf\n",
    "#print PERF_FORMAT_STRING.format(accuracy, precision, recall, f1, f2, display_precision = 5)\n",
    "print 'Accuracy =',accuracy,'Percision =', precision,'Recall =',recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<pre>\n",
    "```\n",
    "\n",
    "| Parameters                             | score              |\n",
    "|----------------------------------------|--------------------|\n",
    "| max_features=2,min_samples_split=5,    | Precision: 0.66667 |\n",
    "|  max_depth = 40,criterion='entropy',   |                    |\n",
    "|    random_state=75                     | Recall:    0.25000 | \n",
    "|-------------------------------------------------------------|\n",
    "| max_features=4,min_samples_split=2,    | Precision: 0.50000 |\n",
    "|  ,max_depth = 10,criterion='gini',     |                    |\n",
    "|    random_state=42                     | Recall:    0.50000 | \n",
    "|-------------------------------------------------------------|\n",
    "| max_features=3,min_samples_split=10,   | Precision: 0.50000 |\n",
    "|  ,max_depth = 50,criterion='entropy',  |                    |\n",
    "|    random_state=72                     | Recall:    0.25000 | \n",
    "|-------------------------------------------------------------|\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "</pre>\n",
    "\n",
    "\n",
    "#### The results from StratifiedShuffleSplit were higher than using just train_test_split. This is because using stratified splits is more accurate than just splitting the data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5:What is validation, and what’s a classic mistake you can make if you do it wrong? How did you validate your analysis?\n",
    "\n",
    "#### Validation allows us to assess how well the chosen algorithm,overfitting is one of the biggest mistakes one can make is therefore to use the same data fro training and testing.\n",
    "####  For validation I'm using provided tester function which performs stratified shuffle split cross validation approach using StratifiedShuffleSplit function from sklearn.cross_validation library.\n",
    "#### Stratified selection is important because it reduces selection bias.Stratifying the entire population before applying random sampling methods helps ensure a sample that accurately reflects the population being studied \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=10,\n",
      "            max_features=5, max_leaf_nodes=None, min_impurity_split=1e-07,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=42,\n",
      "            splitter='best')\n",
      "\tAccuracy: 0.90073\tPrecision: 0.61693\tRecall: 0.67400\tF1: 0.64421\tF2: 0.66176\n",
      "\tTotal predictions: 15000\tTrue positives: 1348\tFalse positives:  837\tFalse negatives:  652\tTrue negatives: 12163\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "### Task 6: Dump your classifier, dataset, and features_list so anyone can\n",
    "### check your results. You do not need to change anything below, but make sure\n",
    "### that the version of poi_id.py that you submit can be run on its own and\n",
    "### generates the necessary .pkl files for validating your results.\n",
    "\n",
    "tester.dump_classifier_and_data(clf, new_dataset, features_list)\n",
    "tester.main() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6:Give at least 2 evaluation metrics, and your average performance for each of them. Explain an interpretation of your metrics that says something human-understandable about your algorithm's performance\n",
    "####  I used Percision & recall as 2 main evaluation metrics , the DecisionTreeClassifier provided the best recall score before tuning.\n",
    "### Average scores of tuned classifier with stratified shuffle  :\n",
    "#### the average Recall i got was :      0.6238\n",
    "#### the average persicion score i got was :  0.65549\n",
    "##### Relatively speaking, looking at the tuned model with a precision score of 0.61, it tells us that if this model predicts 100 POIs, then the chance would be 61 people who are truely POIs and the rest 39 are innocent. On the other hand, with a recall score of 0.67, this model can find 67% of all real POIs in prediction.The F1 score can be interpreted as a weighted average of the precision and recall, where an F1 score reaches its best value at 1 and worst at 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References:\n",
    "\n",
    "Enron Bonuses article : http://www.salon.com/2002/02/08/enron_bonuses/\n",
    "    \n",
    "F1 score on Wikipedia : https://en.wikipedia.org/wiki/F1_score\n",
    "\n",
    "Advantages and Disadvantages of Stratified Random Sampling: http://www.investopedia.com/ask/answers/041615/what-are-advantages-and-disadvantages-stratified-random-sampling.asp#ixzz4jVOqGPan "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
